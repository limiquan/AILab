
# 人工神经网络课程实验结果

## 1.各种训练技巧来提高模型性能

### 1.探究超参数对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100
数据集：CUB_200_2011

#### a.迭代次数的影响

| 迭代次数 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 50 | 0.9923 | 0.8401 |
| 100 | 0.9967 | 0.8454 |
| 150 | 0.9958 | 0.8490 |

#### b.batch_size的影响

| batch_size | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 32 | 0.9998 | 0.8564 |
| 64 | 0.9967 | 0.8454 |
| 128 | 0.9725 | 0.7749 |

#### c.初始学习率的影响

| ini_lr | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 1e-2 | 0.9414 | 0.8160 |
| 1e-3 | 0.9762 | 0.8273 |
| 1e-4 | 0.9967 | 0.8454 |

### 2.探究交叉验证对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100

| 是否使用交叉验证 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9967 | 0.8454 |
| 使用 | 0.9975 | 0.8432 |

### 3.探究早停机制对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100

| 是否使用早停 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9975 | 0.8452 |
| 使用 | 0.9967 | 0.8454 |

## 2.迁移学习

使用预训练模型的权重来进行迁移学习
使用网络：resnet101
batch_size = 64
lr = 1e-4
迭代次数：100
迁移学习策略：Transfer Learning，冻结全部卷积层，只训练前最后的fc层

| 迁移学习使用策略 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9971 | 0.8398 |
| Transfer Learning | 0.8359 | 0.7611 |

可以发现使用迁移学习和不使用迁移学习的收敛速度相近，在30个epoch左右已经稳定下来，
但是迁移学习极大提高了训练速度,因为训练所需的参数大大减少，作为代价，准确率有所下降

## 3.图像生成

使用GAN来生成图像
在训练过程中，我们发现当判别器和生成器的学习率相等时，会出现判别器的收敛速度快于生成器的形况，导致生成图像的质量较差
我们尝试降低判别器的学习率，提高生成器的学习率，发现判别器和生成器的收敛速度较为均衡，但是可能由于训练次数较少，导致生成的图像质量也不是很理想

## 4.对比CNN模型和ViT模型

参数：
batch_size = 64
lr = 1e-4
迭代次数：100

对比各类Resnet模型和ViT模型在CUB_200_2011数据集上的性能，结果如下：
|模型| 训练集acc | 测试集acc | 参数量 |
| ---- | ---- | ---- | ---- |
|vit_b_16| 0.9166 | 0.4937 | 85952456 |
|vit_l_16|  |  | 303506632 |
|resnet50| 0.9967 | 0.8454 | 23917832 |
|resnet101| 0.9971 | 0.8398 | 42909960 |

（放两张图，对比一下resnet101和vit_b_16的收敛速度）
从上表中可以看出，ViT模型的参数量普遍大于resnet模型，这样也导致了在相同的迭代次数之下，ViT模型的收敛速度远比resnet模型要慢，并且训练时间也要更长，在本次实验中，由于vit_l_16模型的参数量过大，导致CUDA内存溢出，因此只能将batch_size减小为16进行实验
ViT是一种基于自注意力机制的模型，具有全局感知的特性，并且ViT模型的自注意力机制不容易实现并行计算，因此在训练过程中需要更长的时间
因此，从总体来看，ViT模型与CNN模型相比，具有更大的参数量，并且收敛速度更慢，训练时间更长，但是同时也拥有了CNN模型所不具备的全局感知特性，能够获取更多的上下文信息。同时，ViT无法利用图像本身具有的尺度、平移不变性和特征局部性等先验知识，必须使用大规模数据集学习高质量的中间表示，适合用于数据集规模较大的场景。因此，当数据集规模较大，并且训练资源较为充足的时候，使用ViT模型可能会获得更好的性能，而当数据集规模较小，或者训练资源有限时，使用CNN模型可能效果更好。

## 5.
