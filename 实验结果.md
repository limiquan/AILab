
# 人工神经网络课程实验结果

## 1.各种训练技巧来提高模型性能

### 1.探究超参数对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100
数据集：CUB_200_2011

#### a.迭代次数的影响

| 迭代次数 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 50 | 0.9923 | 0.8401 |
| 100 | 0.9967 | 0.8454 |
| 150 | 0.9958 | 0.8490 |

#### b.batch_size的影响

| batch_size | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 32 | 0.9998 | 0.8564 |
| 64 | 0.9967 | 0.8454 |
| 128 | 0.9725 | 0.7749 |

#### c.初始学习率的影响

| ini_lr | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 1e-2 | 0.9414 | 0.8160 |
| 1e-3 | 0.9762 | 0.8273 |
| 1e-4 | 0.9967 | 0.8454 |

### 2.探究交叉验证对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100

| 是否使用交叉验证 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9967 | 0.8454 |
| 使用 | 0.9975 | 0.8432 |

### 3.探究早停机制对模型性能的影响

使用网络：resnet50
batch_size = 64
lr = 1e-4
迭代次数：100

| 是否使用早停 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9975 | 0.8452 |
| 使用 | 0.9967 | 0.8454 |

## 2.迁移学习

使用预训练模型的权重来进行迁移学习
使用网络：resnet101
batch_size = 64
lr = 1e-4
迭代次数：100
迁移学习策略：Transfer Learning，冻结全部卷积层，只训练前最后的fc层

| 迁移学习使用策略 | 训练集准确率 | 测试集准确率 |
| ---- | ---- | ---- |
| 不使用 | 0.9971 | 0.8398 |
| Transfer Learning | 0.8359 | 0.7611 |

可以发现使用迁移学习和不使用迁移学习的收敛速度相近，在30个epoch左右已经稳定下来，
但是迁移学习极大提高了训练速度,因为孙连所需的参数大大减少，作为代价，准确率有所下降

## 3.图像生成

使用GAN来生成图像
在训练过程中，我们发现当判别器和生成器的学习率相等时，会出现判别器的收敛速度快于生成器的形况，导致生成图像的质量较差
我们尝试降低判别器的学习率，提高生成器的学习率，发现判别器和生成器的收敛速度较为均衡，但是可能由于训练次数较少，导致生成的图像质量也不是很理想

## 4.对比CNN模型和ViT模型

参数：
batch_size = 64
lr = 1e-4
迭代次数：100

对比各类Resnet模型和ViT模型在CUB_200_2011数据集上的性能，结果如下：
|模型| 训练集acc | 测试集acc |


## 5.
